# -*- coding: utf-8 -*-
"""Case Study 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZxaDBXqOBFdhWpMFXuEkEU1uyO-kScus
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

url='https://drive.google.com/file/d/1xp8eI2r4Gfdj8IngIcncecxpOVHgkHJM/view?usp=sharing'
url='https://drive.google.com/uc?id=' + url.split('/')[-2]
df = pd.read_csv(url)

df

df.duplicated().value_counts() #No duplicate values

df.dtypes

df['emp_length']=df['emp_length'].replace(np.nan,0)
df['emp_length']=df['emp_length'].astype(int)

df.dtypes #changed emp_length to integer

"""
# Describe the dataset and any issues with it.
"""

df.describe

"""# Describing the Dataset:-
There are 10000 records of loans with 55 columns containing information regarding the loan.

There are no duplicated values in the dataset.

There are 10 columns that contains null values.

# Issues Regarding Dataset:-
1. As per the above observations, we can see that there are 10 columns which contains Null values. In order to clean this data, we can perform following operations:- 

  1) In terms of numeric data, we can either replace null values with 0 or with mean value or median value of that column(In this case, I have replaced null values with mean). Over here, we can't replace null values as it would change the context.

  2) In terms of text data, we can delete the rows containing null values if required.
 

2. Another issue is regarding the dataypes. Emp_Length is assigned float but Work experience years cannot be in float. So, we can convert it into integer. Also issue month is assigned object whereas it should be assigned as date.

3. Annual Income Joint and Debt to Income Joint both contains same no. of null Values. So, both the columns are dependent on each other.

4. Currents_Accounts_Delinq contains 0 in all the records. So, we can discard this column as it does not provide much information.

5. Many values in the columns(like verification income joint) contains blank values. we can simply replace it with np.nan i.e. NA values.
"""

df.isnull().sum() #Checking for any null values

df.verification_income_joint=df.verification_income_joint.fillna(np.nan)

df=df.drop('current_accounts_delinq',axis=1)

"""Emp Title - Represents Title of Employee and consists 833 Null values. We don't have to remove this records.

Emp Length - Represents number of years worked as Employee.

State - Represents State of the Employee.

Homeownership - Represents whether home is owned/Mortgage/Rented.

Annual Income = Represents Annual Income of the Employee.

Verified Income = Represents Verified/Source Verified/ Not Verified.

Debt to Income = Represents Debt to Income Ratio.

Annual Income Joint = Represents Joint Annual Income.

Verification Income Joint = Represents Verified/Source Verified/ Not Verified.

Debt to Income Joint = Represents Debt to Income Joint Ratio.

Delinq 2y = Represents Delinquency for 2 years.

# Visuals

# 1. Analyzing Top 10 Employees who are interested in Loans (so that we can target that particular job group for loans).
"""

v1=df.emp_title.value_counts()
v1=v1.to_frame()
v1=v1.reset_index()
v1.columns=['Title','Count']

v1.Count.sort_values(ascending=False)
v1=v1.head(10) #We are considering only top 10 Records to target that customers
v1

plt.figure(figsize=(8,10))
plt.pie(v1.Count,autopct='%1.1f%%',labels=v1.Title)

"""With this analysis, we can target this top 10 job employees as they are the ones who are taking loan more often as per the records.

So, with the help of this visuals, we can target Managers, Owners and Teachers for taking loans.

# 2. Analyzing Top 10 State who are taking loans.
"""

v2=df.state.value_counts()
v2=v2.to_frame()
v2=v2.reset_index()
v2.columns=['State','Count']
v2.Count.sort_values(ascending=False)
v2=v2.head(10) #We are considering only top 10 Records to target that customers
v2

plt.figure(figsize=(8,10))
sns.barplot(data=v2, x='State', y='Count')

"""With this visual, we can see that California, Texas, New york and Florida are the states where people are taking most loans.

# 3. Visualizing Loan Purpose
"""

plt.figure(figsize=(15,8))
ax=sns.countplot(data=df,x='loan_purpose')
for p in ax.patches:
   ax.annotate('{:}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+40.0))

"""Over here, we can see that Debt_Consolidation and credit card loans are the most number of loans offered. So, we can target such customers for the above purposes.

# 4. Loan Interest rates depending on type of Home
"""

plt.figure(figsize=(10,8))
sns.kdeplot(data=df,x='interest_rate',hue='homeownership')

"""Interest rate increases if House is on Mortgage whereas for those who own thier house gets lower interest rate.

# 5. Visualizing Application Type vs Grade wise
"""

plt.rcParams["figure.autolayout"] = True
plt.figure(figsize=(8,10))
order=['A','B','C','D','E','F','G']
ax=sns.countplot(data=df,x='grade',hue='application_type', order=order,ec='black',lw=2)  
for p in ax.patches:
   ax.annotate('{:}'.format(p.get_height()), (p.get_x()+0.05, p.get_height()+40.0))
plt.show()

"""Over here, the number of applications for Grade A,B and C are more in terms of individual and joint account.

# 6. Mean Annual Income getting loans
"""

sns.histplot(df,x='annual_income',binwidth=50000,binrange=(3000,388000))
plt.axvline(x=df.annual_income.mean(),
            color='red')

"""Mean annual income is around 79222 which is plotted as the red line. Different Bins are created to get an idea about the count of annual income.

# 7. Relationship between total debit limit to interest rate
"""

sns.histplot(df, x='interest_rate', y='total_debit_limit')

"""# Create a feature set and create a model which predicts interest_rate using at least 2 algorithms.

# 1. Linear Regression
"""

df.plot.scatter(x='interest_rate',y='annual_income')

sns.histplot(df,x='interest_rate',y='annual_income')

"""Over here, their's one outlier which is present at the top of Test Result. We need to remove it."""

df.drop(725,axis=0,inplace=True)#deleting outlier

df.iloc[725,:]

X=df.loc[:,['interest_rate']]
y=df.loc[:,'annual_income']

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0)
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

train = plt
train.scatter(X_train, y_train, color='red')
train.plot(X_train, regressor.predict(X_train), color='blue')
train.title('Salary VS Interest rate (Test set)')
train.xlabel('Interest')
train.ylabel('Annual Income')
train.show()
train.show()

test = plt
test.scatter(X_test, y_test, color='red')
test.plot(X_train, regressor.predict(X_train), color='blue')
test.title('Salary VS Interest rate (Test set)')
test.xlabel('Interest')
test.ylabel('Annual Income')
test.show()

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

X=df.loc[:,['interest_rate','debt_to_income']]
y=df.loc[:,'annual_income']

X=X.dropna()

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

X=X.astype(int)

y=y.astype(int)

X_train=X_train.to_frame()
y_train=y_train.to_frame()
X_test=X_test.to_frame()

X_train= X_train.values.reshape(-1, 1)

y_train= y_train.values.reshape(-1, 1)
X_test = X_test.values.reshape(-1, 1)

logreg = LogisticRegression()

# fit the model with data
logreg.fit(X_train,y_train)

#
y_pred=logreg.predict(X_test)

from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

